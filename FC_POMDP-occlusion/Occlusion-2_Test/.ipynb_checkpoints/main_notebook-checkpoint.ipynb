{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#### Import own modules ####\n",
    "import sys\n",
    "sys.path.insert(0,'environment/')\n",
    "import q_learning_pomdp\n",
    "import world_pomdp\n",
    "import lateral_agent\n",
    "\n",
    "def vectorize_state(state):\n",
    "    v_x = []\n",
    "    v_y = []\n",
    "    v_v = []\n",
    "\n",
    "    for id_n in range(len(state)):\n",
    "        v_x.append(state[id_n].x)\n",
    "        v_y.append(state[id_n].y)\n",
    "        v_v.append(state[id_n].v)\n",
    "\n",
    "\n",
    "    state_v = np.concatenate((v_x,v_y))\n",
    "    state_v = np.concatenate((state_v, v_v))\n",
    "    return state_v\n",
    "\n",
    "\n",
    "\n",
    "def relative_state(state):\n",
    "\n",
    "    for id_n in range(len(state)-1,-1,-1):\n",
    "        state[id_n].x = state[id_n].x-state[0].x\n",
    "        state[id_n].y = state[id_n].x - state[0].y\n",
    "        state[id_n].v = state[id_n].x - state[0].v\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def processState(states):\n",
    "    return np.reshape(states,[input_dim_v])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Environment parameters ####\n",
    "\n",
    "num_of_cars = 2\n",
    "num_of_lanes = 2\n",
    "track_length = 300\n",
    "speed_limit = 120\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "x_range = 10\n",
    "\n",
    "\n",
    "#### Ego parameters ####\n",
    "\n",
    "ego_lane_init = 1\n",
    "ego_pos_init = 0\n",
    "ego_speed_init = speed_limit\n",
    "x_view = 150\n",
    "y_view = 5\n",
    "\n",
    "#### Network parameters ####\n",
    "\n",
    "input_dim = [x_view*2+1,y_view*2+1]\n",
    "input_dim_v = (x_view*2+1)*(y_view*2+1)*8\n",
    "output_dim = (x_range*num_of_lanes)\n",
    "hidden_units = 50\n",
    "layers = 3\n",
    "clip_value = 7500\n",
    "learning_rate = 0.001\n",
    "buffer_size = 100000\n",
    "batch_size = 64\n",
    "update_freq = 20000\n",
    "kernel_size = [2,2]\n",
    "stride = [2,2]\n",
    "\n",
    "#### RL Parameters ####\n",
    "\n",
    "gamma = 0.99\n",
    "eStart = 1\n",
    "eEnd = 0.1\n",
    "estep = 1500000\n",
    "\n",
    "#### Learning Parameters ####\n",
    "\n",
    "max_train_episodes = 10000\n",
    "pre_train_steps = 50000\n",
    "random_sweep = 3\n",
    "tau = 1\n",
    "\n",
    "\n",
    "#### Environment ####\n",
    "\n",
    "done = False\n",
    "dt = 0.1\n",
    "timestep = 0\n",
    "\n",
    "lateral_controller = lateral_agent.lateral_control(dt)\n",
    "env = world_pomdp.World(num_of_cars,num_of_lanes,track_length,speed_limit,ego_pos_init,ego_lane_init,ego_speed_init,dt,random_seed,x_range)\n",
    "goal_lane = (ego_lane_init - 1) * env.road_width + env.road_width * 0.5\n",
    "goal_lane_prev = goal_lane\n",
    "action = np.zeros(1) # acc/steer\n",
    "\n",
    "#### Plot variables ####\n",
    "max_timestep = 750\n",
    "average_window = 100\n",
    "finished = 0\n",
    "x_ego_list = np.zeros((random_sweep,max_timestep))\n",
    "y_ego_list = np.zeros((random_sweep,max_timestep))\n",
    "y_acc_list = np.zeros((random_sweep,max_timestep))\n",
    "v_ego_list = np.zeros((random_sweep,max_timestep))\n",
    "x_acc_list = np.zeros((random_sweep,max_timestep))\n",
    "reward_list = np.zeros((random_sweep,max_timestep))\n",
    "reward_sum_list = np.zeros((random_sweep,max_train_episodes))\n",
    "reward_average = np.zeros((random_sweep,int(max_train_episodes/average_window)))\n",
    "finished_average = np.zeros((random_sweep,int(max_train_episodes/average_window)))\n",
    "\n",
    "param_id = \"test\"\n",
    "\n",
    "print(\"Start Training!\")\n",
    "\n",
    "for r_seed in range(0,random_sweep):\n",
    "    start = time.time()\n",
    "\n",
    "    random.seed(r_seed)\n",
    "\n",
    "    #### Start training process ####\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    reward_time = []\n",
    "\n",
    "    folder_path = './training/'\n",
    "\n",
    "    path_save = folder_path+ \"testing_init_2/\"\n",
    "\n",
    "    ## Set up networks ##\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    mainQN = q_learning_pomdp.qnetwork(input_dim_v, output_dim, hidden_units, layers, learning_rate, clip_value,kernel_size,stride)\n",
    "    targetQN = q_learning_pomdp.qnetwork(input_dim_v, output_dim, hidden_units, layers, learning_rate, clip_value,kernel_size,stride)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "    trainables = tf.trainable_variables()\n",
    "\n",
    "    targetOps = q_learning_pomdp.updateNetwork(trainables, tau)\n",
    "\n",
    "    load_model = False\n",
    "    ## Create replay buffer ##\n",
    "    exp_buffer = q_learning_pomdp.replay_buffer(buffer_size)\n",
    "\n",
    "    ## Randomness of actions ##\n",
    "    epsilon = eStart\n",
    "    stepDrop = (eStart - eEnd) / estep\n",
    "\n",
    "    ## Further Variables\n",
    "    done =False\n",
    "    total_steps = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        start = time.time()\n",
    "        for episode in range(max_train_episodes):\n",
    "            episode_buffer = q_learning_pomdp.replay_buffer(buffer_size)\n",
    "            env = world_pomdp.World(num_of_cars, num_of_lanes, track_length, speed_limit, ego_pos_init, ego_lane_init,\n",
    "                              ego_speed_init,\n",
    "                              dt, r_seed, x_range)\n",
    "\n",
    "            observation = env.field_of_view()\n",
    "            observation_v = processState(observation)\n",
    "\n",
    "            reward_sum = 0\n",
    "            timestep = 0\n",
    "            action = 0\n",
    "            done = False\n",
    "\n",
    "            while done == False:\n",
    "                if total_steps % 5 == 0:\n",
    "                    if (np.random.random() < epsilon or total_steps < pre_train_steps):\n",
    "                        action = random.randint(0,num_of_lanes*x_range-1)\n",
    "                    else:\n",
    "                        action = sess.run(mainQN.action_pred,feed_dict={mainQN.input_scalar:[observation_v]})\n",
    "\n",
    "                state1, reward,done = env.step(action)\n",
    "                observation_1 = env.field_of_view()\n",
    "                observation_1_v = processState(observation_1)\n",
    "\n",
    "\n",
    "\n",
    "                total_steps += 1\n",
    "\n",
    "                #episode_buffer.add(np.reshape(np.array([state_v,action,reward,state1_v,done]),[1,5]))\n",
    "                episode_buffer.add(np.reshape(np.array([observation_v, action, reward, observation_1_v, done]), [1, 5]))\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if epsilon > eEnd:\n",
    "                        epsilon-=stepDrop\n",
    "\n",
    "                    trainBatch = exp_buffer.sample(batch_size)\n",
    "                    ## Calculate Q-Value: Q = r(s,a) + gamma*Q(s1,a_max)\n",
    "                    # Use of the main network to predict the action a_max\n",
    "                    action_max = sess.run(mainQN.action_pred,feed_dict={mainQN.input_scalar:np.vstack(trainBatch[:,3])})\n",
    "                    Qt1_vec = sess.run(targetQN.output_q_predict,feed_dict={targetQN.input_scalar: np.vstack(trainBatch[:,3])}) #Q-values for s1\n",
    "\n",
    "                    end_multiplier = -(trainBatch[:,4]-1)\n",
    "                    Qt1 = Qt1_vec[range(batch_size),action_max] # select Q(s1,a_max)\n",
    "\n",
    "                    # Q = r(s,a) + gamma*Q(s1,a_max)\n",
    "                    Q_gt = trainBatch[:,2] + gamma*Qt1*end_multiplier\n",
    "\n",
    "                    #Optimize network\n",
    "                    _ = sess.run(mainQN.update,feed_dict={mainQN.input_scalar:np.vstack(trainBatch[:,0]),mainQN.q_gt:Q_gt,mainQN.actions:trainBatch[:,1]})\n",
    "\n",
    "                    ## Update target network ##\n",
    "                    if total_steps % update_freq == 0:\n",
    "                        print(\"Update target network!\")\n",
    "                        q_learning_pomdp.updateTarget(targetOps,sess)\n",
    "\n",
    "                reward_sum+=reward\n",
    "\n",
    "                observation_v = observation_1_v\n",
    "                #env.render()\n",
    "            exp_buffer.add(episode_buffer.buffer)\n",
    "            reward_sum_list[r_seed, episode] = reward_sum\n",
    "            end = time.time()\n",
    "            #print(\"Time for one episode: \",end-start,\" Episode: \",episode)\n",
    "            if env.success == True:\n",
    "                finished += 1\n",
    "\n",
    "            if episode % 400 == 0:\n",
    "                save_path = saver.save(sess,path_save+\"modelRL_\"+str(r_seed)+\"_\"+str(episode)+\".ckpt\")\n",
    "                print(\"Model saved in: \",save_path)\n",
    "            if episode % average_window == 0:\n",
    "                if episode > 1:\n",
    "                    print(\"Total steps: \", total_steps, \" Episode: \", episode,\n",
    "                          \" Average reward over \" + str(average_window) + \" Episodes: \",\n",
    "                          np.mean(reward_sum_list[r_seed, episode - average_window:episode]), \"Finished: \", finished,\n",
    "                          \"/\" + str(average_window) + \"\", \" Episode:\", episode, \" Epsilon: \", epsilon)\n",
    "                    #### Write to file ####\n",
    "                    file = open(path_save + 'training_process' + str(r_seed) + '.txt', 'a')\n",
    "                    file.write(\"Total steps: \"+str(total_steps)+\" Episode: \"+ str(episode)+\" Average reward over \"+str(average_window)+\" Episodes: \"+\n",
    "                          str(np.mean(reward_sum_list[r_seed,episode-average_window:episode]))+\"Finished: \"+str(finished)+\"/\"+str(average_window)+\" Epsilon: \"+str(epsilon)+\"\\n\")\n",
    "\n",
    "                    file.close()\n",
    "                    #### Close File ####\n",
    "\n",
    "                    reward_average[r_seed, int(episode / average_window)] = np.mean(reward_sum_list[r_seed,episode-average_window:episode])\n",
    "                    finished_average[r_seed, int(episode / average_window)] = finished\n",
    "                    finished = 0\n",
    "\n",
    "            if r_seed == 0 and episode == 1:  # Only write for first time\n",
    "\n",
    "                file = open(path_save + 'params_' + str(param_id) + '.txt', 'w')\n",
    "                # file = open(complete_file, 'w')\n",
    "                file.write('NETWORK PARAMETERS: \\n\\n')\n",
    "                file.write('Layers: ' + str(layers) + '\\n')\n",
    "                file.write('Hidden units: ' + str(hidden_units) + '\\n')\n",
    "                file.write('Learning rate: ' + str(learning_rate) + '\\n')\n",
    "                file.write('Buffer size: ' + str(buffer_size) + '\\n')\n",
    "                file.write('Pre_train_steps: ' + str(pre_train_steps) + '\\n')\n",
    "                file.write('Batch_size: ' + str(batch_size) + '\\n')\n",
    "                file.write('Update frequency: ' + str(update_freq) + '\\n')\n",
    "                file.write('Tau: ' + str(tau) + '\\n\\n')\n",
    "\n",
    "                file.write('RL PARAMETERS: \\n\\n')\n",
    "                file.write('Gamma: ' + str(gamma) + '\\n')\n",
    "                file.write('Epsilon start: ' + str(eStart) + '\\n')\n",
    "                file.write('Epsilon end: ' + str(eEnd) + '\\n')\n",
    "                file.write('Epsilon steps: ' + str(estep) + '\\n\\n')\n",
    "\n",
    "                file.write('SCENARIO PARAMETERS: \\n\\n')\n",
    "                file.write('Cars: ' + str(num_of_cars) + '\\n')\n",
    "                file.write('Lanes: ' + str(num_of_lanes) + '\\n')\n",
    "                file.write('Ego speed init: ' + str(ego_speed_init) + '\\n')\n",
    "                file.write('Ego pos init: ' + str(ego_pos_init) + '\\n')\n",
    "                file.write('Ego lane init: ' + str(ego_lane_init) + '\\n')\n",
    "                file.write('Non-Ego tracklength: ' + str(track_length) + \"\\n\\n\\n\")\n",
    "\n",
    "                file.write('REMARKS: Initial test with new state')\n",
    "\n",
    "                file.close()\n",
    "\n",
    "\n",
    "        final_save_path = saver.save(sess,path_save+\"random_\"+str(r_seed)+\"_\"  + \"Final.ckpt\")\n",
    "        print(\"Model saved in: %s\",final_save_path)\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Elapsed time for one cycle: \", end-start)\n",
    "        file = open(path_save + 'training_process' + str(r_seed) + '.txt', 'a')\n",
    "        file.write(\"Elapsed Time for one random seed:\" + str(end-start)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(4)\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.set_title(\"Reward over time\")\n",
    "ax.set_xlabel(\"epsisode/100\")\n",
    "ax.set_ylabel(\"reward\")\n",
    "ax.grid()\n",
    "sns.tsplot(reward_average)\n",
    "manager = plt.get_current_fig_manager()\n",
    "manager.resize(*manager.window.maxsize())\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_save + 'reward' + '.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(5)\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.set_title(\"Success time\")\n",
    "ax.set_xlabel(\"epsiode/100\")\n",
    "ax.set_ylabel(\"Finished Episodes/100\")\n",
    "ax.grid()\n",
    "sns.tsplot(finished_average)\n",
    "manager = plt.get_current_fig_manager()\n",
    "manager.resize(*manager.window.maxsize())\n",
    "# plt.show(block=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_save + 'finished' + '.png')\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
