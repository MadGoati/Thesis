NETWORK PARAMETERS: 

Layers: 3
Hidden units: 50
Learning rate: 0.001
Buffer size: 100000
Pre_train_steps: 100000
Batch_size: 32
Update frequency: 20000
Tau: 1

RL PARAMETERS: 

Gamma: 0.99
Epsilon start: 1
Epsilon end: 0.1
Epsilon steps: 1500000

SCENARIO PARAMETERS: 

Cars: 2
Lanes: 2
Ego speed init: 120
Ego pos init: 0
Ego lane init: 1
Non-Ego tracklength: 300


REMARKS: Simple Overtake Scenario, No coming traffic, Long Training 



self.reward = 0                             self.reward -= (self.y_acc ** 2) * 0.12                             self.reward -= self.x_acc ** 2  # x_acc                             self.reward -= (self.speed_limit - self.vehicle_list[0].v) * 2                             self.lateral_dist = self.vehicle_list[0].y - 2                            self.reward += (1.375 * self.lateral_dist ** 2 - 6.25 * self.lateral_dist + 5)                            if self.vehicle_list[0].y in {2, 6, 10, 14, 18, 22}:                            self.reward += 1\ ### Velocity ###                            non_ego_avg_v = 0                            for n in range(0, self.n_cars):                                 non_ego_avg_v += self.vehicle_list[n + 1].v                            non_ego_avg_v = non_ego_avg_v / self.n_cars                            non_ego_delta_v = self.non_ego_limit - non_ego_avg_v                            ### Add Global part ###                            self.reward -= 0.1 * non_ego_delta_v ** 2                            vehicle_list_sec = [vehicle for vehicle in self.vehicle_list if self.vehicle_list[0].x < vehicle.x]                            if (len(vehicle_list_sec) == 0 and self.vehicle_list[0].y == (1 - 1) * self.road_width + self.road_width * 0.5):                                self.reward += 1000                            self.done = True                            self.success = True                            return self.reward